import glob
import random
import os
import sys
import warnings
import numpy as np
from PIL import Image

import imgaug.augmenters as iaa
from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage
from imgaug.augmentables.segmaps import SegmentationMapsOnImage

import torch
import torch.nn.functional as F

from torch.utils.data import Dataset
from .utils import xywh2xyxy_np
import torchvision.transforms as transforms



def pad_to_square(img, pad_value):
    c, h, w = img.shape
    dim_diff = np.abs(h - w)
    # (upper / left) padding and (lower / right) padding
    pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2
    # Determine padding
    pad = (0, 0, pad1, pad2) if h <= w else (pad1, pad2, 0, 0)
    # Add padding
    img = F.pad(img, pad, "constant", value=pad_value)

    return img, pad


def resize(image, size):
    image = F.interpolate(image.unsqueeze(0), size=size, mode="nearest").squeeze(0)
    return image


def random_resize(images, min_size=288, max_size=448):
    new_size = random.sample(list(range(min_size, max_size + 1, 32)), 1)[0]
    images = F.interpolate(images, size=new_size, mode="nearest")
    return images


class ImageFolder(Dataset):
    def __init__(self, folder_path, img_size=416):
        self.files = sorted(glob.glob("%s/*.*" % folder_path))
        self.img_size = img_size

    def __getitem__(self, index):
        img_path = self.files[index % len(self.files)]
        # Extract image as PyTorch tensor
        img = transforms.ToTensor()(Image.open(img_path))
        # Pad to square resolution
        img, _ = pad_to_square(img, 0)
        # Resize
        img = resize(img, self.img_size)

        return img_path, img

    def __len__(self):
        return len(self.files)


class ListDataset(Dataset):
    def __init__(self, list_path, img_size=416, multiscale=True, transform=None):
        with open(list_path, "r") as file:
            self.img_files = file.readlines()

        self.label_files = [
            path.replace("images", "labels").replace(".png", ".txt").replace(".jpg", ".txt")
            for path in self.img_files
        ]
        self.mask_files = [
            path.replace("images", "masks").replace(".jpg", ".png")
            for path in self.img_files
        ]
        self.img_size = img_size
        self.max_objects = 100
        self.multiscale = multiscale
        self.min_size = self.img_size - 3 * 32
        self.max_size = self.img_size + 3 * 32
        self.batch_count = 0
        self.transform = transform

    def __getitem__(self, index):

        # ---------
        #  Image
        # ---------

        img_path = self.img_files[index % len(self.img_files)].rstrip()

        img = np.array(Image.open(img_path).convert('RGB'), dtype=np.uint8)

        # ---------
        #  Label
        # ---------

        label_path = self.label_files[index % len(self.img_files)].rstrip()

        # Ignore warning if file is empty
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            boxes = np.loadtxt(label_path).reshape(-1, 5)
        
        # ---------
        #  Segmentation Mask
        # ---------

        mask_path = self.mask_files[index % len(self.img_files)].rstrip()

        # Extract image as PyTorch tensor
        mask = np.array(Image.open(mask_path).convert('RGB'), dtype=np.uint8) * 255

        if self.transform:
            img, bb_targets, mask_targets = self.transform(
                (img, boxes, mask)
            )

        return img_path, img, bb_targets, mask_targets

    def collate_fn(self, batch):
        self.batch_count += 1
        paths, imgs, bb_targets, mask_targets= list(zip(*batch))
        
        # Selects new image size every tenth batch
        if self.multiscale and self.batch_count % 10 == 0:
            self.img_size = random.choice(range(self.min_size, self.max_size + 1, 32))
        
        # Resize images to input shape
        imgs = torch.stack([resize(img, self.img_size) for img in imgs])

        # Add sample index to targets
        for i, boxes in enumerate(bb_targets):
            boxes[:, 0] = i
        bb_targets = torch.cat(bb_targets, 0)
        

        # Stack masks and drop the 2 duplicated channels 
        mask_targets = torch.stack([resize(mask, self.img_size)[0] for mask in mask_targets])
        # Reshape mask and convert to long
        mask_targets = mask_targets.reshape(-1, 1, self.img_size, self.img_size).long()

        return paths, imgs, bb_targets, mask_targets

    def __len__(self):
        return len(self.img_files)
